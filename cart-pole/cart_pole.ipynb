{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cart Pole\n",
    "\n",
    "I adapt [\"Actor Critic Method\" by Apoorv Nandan](https://keras.io/examples/rl/actor_critic_cartpole/) for the [OpenAI Gym CartPole-v1 task](https://gymnasium.farama.org/environments/classic_control/cart_pole/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Actor Critic method involves two components:\n",
    "\n",
    "* The *actor* computes a probability for each action in the state space.\n",
    "* The *critic* computes the sum of all rewards the agent expects to receive in the future.\n",
    "\n",
    "The agent learns to select actions that maximize the rewards it expects it will receive.\n",
    "\n",
    "In the CartPole-v1 task, the agent can take two actions: push cart to the left (0) and push cart to the right (1). An observation $(x, v, \\theta, \\omega)$ consists of position $x$, velocity $v$, pole angle $\\theta$, and angular velocity $\\omega$. The agent is awarded +1 for each step taken, since the goal is to keep the pole upright as long as possible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, the required libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras import ops, Model\n",
    "from tensorflow.keras.layers import Dense, Input\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "DISCOUNT_FACTOR = 0.99\n",
    "STEPS_PER_EPISODE = 10_000\n",
    "env = gym.make('CartPole-v1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The actor and the critic share an input and hidden layer:\n",
    "![Diagram of Model](./Cart%20Pole%20Actor-Critic%20Model.svg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_INPUTS = 4\n",
    "NUM_ACTIONS = 2\n",
    "NUM_HIDDEN = 128\n",
    "EPS = np.finfo(np.float32).eps\n",
    "\n",
    "inputs = Input(shape=(NUM_INPUTS,))\n",
    "common = Dense(NUM_HIDDEN, activation='relu')(inputs)\n",
    "action = Dense(NUM_ACTIONS, activation='softmax')(common)\n",
    "critic = Dense(1)(common)\n",
    "\n",
    "model = Model(inputs=inputs, outputs=[action, critic])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running reward at episode 10: 10.18\n",
      "Running reward at episode 20: 25.27\n",
      "Running reward at episode 30: 42.34\n",
      "Running reward at episode 40: 40.89\n",
      "Running reward at episode 50: 40.25\n",
      "Running reward at episode 60: 49.19\n",
      "Running reward at episode 70: 48.30\n",
      "Running reward at episode 80: 78.89\n",
      "Running reward at episode 90: 90.70\n",
      "Running reward at episode 100: 128.60\n",
      "Running reward at episode 110: 134.02\n",
      "Running reward at episode 120: 161.16\n",
      "Running reward at episode 130: 170.25\n",
      "Running reward at episode 140: 294.16\n",
      "Running reward at episode 150: 204.87\n",
      "Running reward at episode 160: 171.55\n",
      "Running reward at episode 170: 346.13\n",
      "Running reward at episode 180: 251.56\n",
      "Running reward at episode 190: 569.21\n",
      "Solved at episode 193!\n"
     ]
    }
   ],
   "source": [
    "optimizer = keras.optimizers.Adam(learning_rate=0.01)\n",
    "# model.compile(optimizer=optimizer)\n",
    "critic_loss = keras.losses.Huber()\n",
    "running_reward = 0\n",
    "episode_count = 0\n",
    "\n",
    "while True:\n",
    "    state, _ = env.reset()\n",
    "    action_probs_history = []\n",
    "    expected_return_history = []\n",
    "    rewards_history = []\n",
    "    episode_reward = 0\n",
    "    with tf.GradientTape() as tape:\n",
    "        for timestep in range(1, STEPS_PER_EPISODE):\n",
    "            # env.render()\n",
    "\n",
    "            state = tf.convert_to_tensor(state)\n",
    "            state = tf.expand_dims(state, 0)\n",
    "\n",
    "            action_probs, expected_return = model(state)\n",
    "            action = np.random.choice(NUM_ACTIONS, p=np.squeeze(action_probs))\n",
    "            action_probs_history.append(ops.log(action_probs[0, action]))\n",
    "            expected_return_history.append(expected_return[0, 0])\n",
    "\n",
    "            state, reward, done, _, _ = env.step(action)\n",
    "            rewards_history.append(reward)\n",
    "            episode_reward += reward\n",
    "\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "        running_reward = 0.05 * episode_reward + (1 - 0.05) * running_reward\n",
    "\n",
    "        # The return at a given time step is the sum of all future\n",
    "        # rewards [..., r2, r1, r0] weighted iteratively by the\n",
    "        # discount factor:\n",
    "        #     returns[0] = r0\n",
    "        #     returns[1] = r1 + Y*r0 = r1 + Y * returns[0]\n",
    "        #     returns[2] = r2 + Y*(r1 + Y*r0) = r2 + Y * returns[1]\n",
    "        returns = np.zeros(len(rewards_history))\n",
    "        discounted_return = 0\n",
    "        for i in range(len(returns)):\n",
    "            discounted_return = rewards_history[-1 - i] + DISCOUNT_FACTOR * discounted_return\n",
    "            returns[-1 - i] = discounted_return\n",
    "\n",
    "        # Normalize by computing the Z-score (x - mean) / stdev.\n",
    "        returns = (returns - np.mean(returns)) / (np.std(returns) + EPS)\n",
    "\n",
    "        actor_loss = 0.\n",
    "        critic_loss_ = 0.\n",
    "        for action_prob, expected, return_ in zip(action_probs_history, expected_return_history, returns):\n",
    "            diff = return_ - expected\n",
    "            actor_loss -= action_prob * diff\n",
    "            critic_loss_ += critic_loss(\n",
    "                ops.expand_dims(expected, 0),\n",
    "                ops.expand_dims(return_, 0)\n",
    "            )\n",
    "\n",
    "        cost = actor_loss + critic_loss_\n",
    "        grads = tape.gradient(cost, model.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "\n",
    "    episode_count += 1\n",
    "    if episode_count % 10 == 0:\n",
    "        print(f'Running reward at episode {episode_count}: {running_reward:.2f}')\n",
    "\n",
    "    if running_reward > 1000:\n",
    "        print(f'Solved at episode {episode_count}!')\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once I trained the model, the cart was able to balance the pole pretty much indefinitely:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "os.environ['SDL_VIDEODRIVER'] = 'dummy'\n",
    "from IPython.display import clear_output\n",
    "from tensorflow.keras.saving import load_model\n",
    "\n",
    "def choose_action(self, state):\n",
    "    state = tf.convert_to_tensor(state)\n",
    "    state = tf.expand_dims(state, 0)\n",
    "    action_probs, _ = model(state)\n",
    "    return np.random.choice(2, p=np.squeeze(action_probs))\n",
    "\n",
    "env = gym.make('CartPole-v1', render_mode='rgb_array')\n",
    "\n",
    "agent = Agent('cart_pole.keras')\n",
    "state, _ = env.reset()\n",
    "done = False\n",
    "while not done:\n",
    "    clear_output(wait=True)\n",
    "    frame = env.render()\n",
    "    plt.imshow(frame)\n",
    "    plt.show()\n",
    "\n",
    "    action = agent.choose_action(state)\n",
    "    state, _, done, _, _ = env.step(action)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
