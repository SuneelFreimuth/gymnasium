{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cart Pole\n",
    "\n",
    "I adapt [\"Actor Critic Method\" by Apporv Nandan](https://keras.io/examples/rl/actor_critic_cartpole/) for the [OpenAI Gym CartPole-v1 task](https://gymnasium.farama.org/environments/classic_control/cart_pole/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Actor Critic method involves two components:\n",
    "\n",
    "* The *actor* computes a probability for each action in the state space.\n",
    "* The *critic* computes the sum of all rewards the agent expects to receive in the future.\n",
    "\n",
    "The agent learns to select actions that maximize the rewards it expects it will receive.\n",
    "\n",
    "In the CartPole-v1 task, the agent can take two actions: push cart to the left (0) and push cart to the right (1). An observation $(x, v, \\theta, \\omega)$ consists of position $x$, velocity $v$, pole angle $\\theta$, and angular velocity $\\omega$. The agent is awarded +1 for each step taken, since the goal is to keep the pole upright as long as possible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, the required libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras import ops, Model\n",
    "from tensorflow.keras.layers import Dense, Input\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DISCOUNT_FACTOR = 0.99\n",
    "STEPS_PER_EPISODE = 10_000\n",
    "env = gym.make('CartPole-v1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The actor and the critic share an input and hidden layer:\n",
    "![Diagram of Model](./Cart%20Pole%20Actor-Critic%20Model.svg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_INPUTS = 4\n",
    "NUM_ACTIONS = 2\n",
    "NUM_HIDDEN = 128\n",
    "EPS = np.finfo(np.float32).eps\n",
    "\n",
    "inputs = Input(shape=(NUM_INPUTS,))\n",
    "common = Dense(NUM_HIDDEN, activation='relu')(inputs)\n",
    "action = Dense(NUM_ACTIONS, activation='softmax')(common)\n",
    "critic = Dense(1)(common)\n",
    "\n",
    "model = Model(inputs=inputs, outputs=[action, critic])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running reward at episode 10: 8.92\n",
      "Running reward at episode 20: 15.75\n",
      "Running reward at episode 30: 24.61\n",
      "Running reward at episode 40: 36.09\n",
      "Running reward at episode 50: 41.26\n",
      "Running reward at episode 60: 61.43\n",
      "Running reward at episode 70: 91.03\n",
      "Running reward at episode 80: 93.72\n",
      "Running reward at episode 90: 146.43\n",
      "Running reward at episode 100: 145.74\n",
      "Running reward at episode 110: 207.56\n",
      "Running reward at episode 120: 245.33\n",
      "Running reward at episode 130: 215.40\n",
      "Running reward at episode 140: 167.05\n",
      "Running reward at episode 150: 138.44\n",
      "Running reward at episode 160: 154.78\n",
      "Solved at episode 162!\n"
     ]
    }
   ],
   "source": [
    "optimizer = keras.optimizers.Adam(learning_rate=0.01)\n",
    "# model.compile(optimizer=optimizer)\n",
    "critic_loss = keras.losses.Huber()\n",
    "running_reward = 0\n",
    "episode_count = 0\n",
    "\n",
    "while True:\n",
    "    state, _ = env.reset()\n",
    "    action_probs_history = []\n",
    "    expected_return_history = []\n",
    "    rewards_history = []\n",
    "    episode_reward = 0\n",
    "    with tf.GradientTape() as tape:\n",
    "        for timestep in range(1, STEPS_PER_EPISODE):\n",
    "            # env.render()\n",
    "\n",
    "            state = tf.convert_to_tensor(state)\n",
    "            state = tf.expand_dims(state, 0)\n",
    "\n",
    "            action_probs, expected_return = model(state)\n",
    "            action = np.random.choice(NUM_ACTIONS, p=np.squeeze(action_probs))\n",
    "            action_probs_history.append(ops.log(action_probs[0, action]))\n",
    "            expected_return_history.append(expected_return[0, 0])\n",
    "\n",
    "            state, reward, done, _, _ = env.step(action)\n",
    "            rewards_history.append(reward)\n",
    "            episode_reward += reward\n",
    "\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "        running_reward = 0.05 * episode_reward + (1 - 0.05) * running_reward\n",
    "\n",
    "        # The return at a given time step is the sum of all future\n",
    "        # rewards [..., r2, r1, r0] weighted iteratively by the\n",
    "        # discount factor:\n",
    "        #     returns[0] = r0\n",
    "        #     returns[1] = r1 + Y*r0 = r1 + Y * returns[0]\n",
    "        #     returns[2] = r2 + Y*(r1 + Y*r0) = r2 + Y * returns[1]\n",
    "        returns = np.zeros(len(rewards_history))\n",
    "        discounted_return = 0\n",
    "        for i in range(len(returns)):\n",
    "            discounted_return = rewards_history[-1 - i] + DISCOUNT_FACTOR * discounted_return\n",
    "            returns[-1 - i] = discounted_return\n",
    "\n",
    "        # Normalize by computing the Z-score (x - mean) / stdev.\n",
    "        returns = (returns - np.mean(returns)) / (np.std(returns) + EPS)\n",
    "\n",
    "\n",
    "        # returns = []\n",
    "        # discounted_sum = 0\n",
    "        # for r in rewards_history[::-1]:\n",
    "        #     discounted_sum = r + DISCOUNT_FACTOR * discounted_sum\n",
    "        #     returns.insert(0, discounted_sum)\n",
    "\n",
    "        # # Normalize\n",
    "        # returns = np.array(returns)\n",
    "        # returns = (returns - np.mean(returns)) / (np.std(returns) + EPS)\n",
    "        # returns = returns.tolist()\n",
    "\n",
    "        actor_loss = 0.\n",
    "        critic_loss_ = 0.\n",
    "        for action_prob, value, return_ in zip(action_probs_history, expected_return_history, returns):\n",
    "            diff = return_ - value\n",
    "            actor_loss -= action_prob * diff\n",
    "            critic_loss_ += critic_loss(\n",
    "                ops.expand_dims(value, 0),\n",
    "                ops.expand_dims(return_, 0)\n",
    "            )\n",
    "\n",
    "        cost = actor_loss + critic_loss_\n",
    "        grads = tape.gradient(cost, model.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "\n",
    "    episode_count += 1\n",
    "    if episode_count % 10 == 0:\n",
    "        print(f'Running reward at episode {episode_count}: {running_reward:.2f}')\n",
    "\n",
    "    if running_reward > 500:\n",
    "        print(f'Solved at episode {episode_count}!')\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('cart_pole.keras')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
